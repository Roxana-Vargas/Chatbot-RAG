from typing import Dict, List, Any
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    answer_similarity,
    answer_correctness
)
from datasets import Dataset
from src.utils.logger import logger

# List of metrics for evaluation
METRICS = [
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    answer_similarity,
    answer_correctness
]

def prepare_dataset(question: str, ground_truth: str, answer: str, contexts: List[str]) -> Dataset:
    """Prepares the dataset for evaluation."""
    data = {
        "question": [question],
        "answer": [answer],
        "contexts": [contexts],
        "ground_truth": [ground_truth]
    }
    return Dataset.from_dict(data)


def compute_evaluation_metrics(dataset: Dataset) -> Dict[str, float]:
    """Computes the evaluation metrics and returns a formatted dictionary."""
    result = evaluate(dataset=dataset, metrics=METRICS)

    def get_metric_value(metric_name: str) -> float:
        """Extracts and rounds the metric value from the result."""
        return round(result[metric_name][0], 2)

    return {metric.name: get_metric_value(metric.name) for metric in METRICS}


def evaluate_question(question: str, ground_truth: str, answer: str, contexts: List[str]) -> Dict[str, Any]:
    """
    Evaluates a question using the RAGAS evaluation model.

    Args:
        question (str): The question to evaluate.
        ground_truth (str): The expected correct answer.
        answer (str): The answer generated by the model.
        contexts (list): List of contexts related to the question.

    Returns:
        dict: Evaluation results with the specified metrics.
    """
    dataset = prepare_dataset(question, ground_truth, answer, contexts)
    evaluation_metrics = compute_evaluation_metrics(dataset)

    logger.info(f"Evaluation result: {evaluation_metrics}")

    return {
        "input_data": {
            "question": question,
            "ground_truth": ground_truth,
            "answer": answer,
            "contexts": contexts
        },
        "metrics": evaluation_metrics
    }
